{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f240bb-a138-4d0b-8b3f-910c5a919d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyGithub in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (2.32.3)\n",
      "Requirement already satisfied: pyjwt[crypto]>=2.4.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (2.3.0)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (1.2.18)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\deba prakash\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.4.1 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deba prakash\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.14.0->PyGithub) (2025.1.31)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Deprecated->PyGithub) (1.17.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\deba prakash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"
     ]
    }
   ],
   "source": [
    "pip install PyGithub pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af35d48-5556-4125-9325-271007778819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: PRAKASH-14A\n"
     ]
    }
   ],
   "source": [
    "from github import Github\n",
    " # Replace with your GitHub token\n",
    "TOKEN = \"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\"\n",
    " # Authenticate with GitHub\n",
    "g = Github(TOKEN)\n",
    " # Test authentication by printing your account info\n",
    "user = g.get_user()\n",
    "print(\"Authenticated as:\", user.login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc317a7-5b5a-42f9-a76a-be4dc7084b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Name: minimaxir/big-list-of-naughty-strings\n",
      "Stars: 46973\n",
      "Description: The Big List of Naughty Strings is a list of strings which have a high probability of causing issues when used as user-input data.\n",
      "\n",
      "Repository Name: google-research/bert\n",
      "Stars: 38681\n",
      "Description: TensorFlow code and pre-trained models for BERT\n",
      "\n",
      "Repository Name: feder-cr/Jobs_Applier_AI_Agent_AIHawk\n",
      "Stars: 27225\n",
      "Description: Jobs_Applier_AI_Agent_AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.\n",
      "\n",
      "Repository Name: trekhleb/learn-python\n",
      "Stars: 16681\n",
      "Description: ðŸ“š Playground and cheatsheet for learning Python. Collection of Python scripts that are split by topics and contain code examples with explanations.\n",
      "\n",
      "Repository Name: cool-RR/PySnooper\n",
      "Stars: 16430\n",
      "Description: Never use print for debugging again\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Define your search query for Python repositories with more than 100 stars\n",
    "query = \"language:python stars:>100 size:<500\"  # Only repos with size < 500 KB\n",
    "repos = g.search_repositories(query=query)\n",
    " # Print basic information about the first 5 repositories\n",
    "for repo in repos[:5]:  # Only displaying the top 5 results\n",
    "     print(f\"Repository Name: {repo.full_name}\")\n",
    "     print(f\"Stars: {repo.stargazers_count}\")\n",
    "     print(f\"Description: {repo.description}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de176fc-9b50-4895-a58c-eef05d722a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching contents of repository: minimaxir/big-list-of-naughty-strings\n",
      "Fetching contents of repository: google-research/bert\n",
      "Collected 12 Python files.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "code_samples = []\n",
    "max_repos = 3  # Limit to 3 repositories\n",
    "max_files = 10  # Limit to 10 .py files\n",
    "max_depth = 1   # Limit folder depth\n",
    "\n",
    "def fetch_contents(repo, path=\"\", depth=0):\n",
    "    if depth > max_depth or len(code_samples) >= max_files:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        contents = repo.get_contents(path)\n",
    "        result = []\n",
    "        \n",
    "        for file_content in contents:\n",
    "            if file_content.type == \"dir\":\n",
    "                result.extend(fetch_contents(repo, file_content.path, depth + 1))\n",
    "            elif file_content.name.endswith(\".py\"):\n",
    "                result.append({\n",
    "                    \"repo\": repo.full_name,\n",
    "                    \"file_name\": file_content.path,\n",
    "                    \"code\": file_content.decoded_content.decode(\"utf-8\", errors=\"ignore\")\n",
    "                })\n",
    "                if len(result) >= max_files:\n",
    "                    break\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching files from {repo.full_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Limit repository search\n",
    "for repo in repos[:max_repos]:\n",
    "    if len(code_samples) >= max_files:\n",
    "        break\n",
    "    print(f\"Fetching contents of repository: {repo.full_name}\")\n",
    "    code_samples.extend(fetch_contents(repo))\n",
    "    time.sleep(1)  # Delay to avoid rate limiting\n",
    "\n",
    "print(f\"Collected {len(code_samples)} Python files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164fd23b-9284-4697-a44b-c6407e485b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: minimaxir/big-list-of-naughty-strings\n",
      "File Name: naughtystrings/__init__.py\n",
      "Code Snippet:\n",
      "import os\n",
      "\n",
      "\n",
      "FILEPATH = os.path.join(\n",
      "    os.path.abspath(os.path.dirname(os.path.dirname(__file__)))...\n",
      "--------------------------------------------------\n",
      "Repository: minimaxir/big-list-of-naughty-strings\n",
      "File Name: scripts/txt_to_json.py\n",
      "Code Snippet:\n",
      "###\tQuick Python Script to convert the Big List of Naughty Strings into a JSON file\n",
      "### \n",
      "###\tBy Max ...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: __init__.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: create_pretraining_data.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: extract_features.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: modeling.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: modeling_test.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: optimization.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: optimization_test.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: run_classifier.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: run_classifier_with_tfhub.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n",
      "Repository: google-research/bert\n",
      "File Name: run_pretraining.py\n",
      "Code Snippet:\n",
      "# coding=utf-8\n",
      "# Copyright 2018 The Google AI Language Team Authors.\n",
      "#\n",
      "# Licensed under the Apache L...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "if code_samples:\n",
    "    for sample in code_samples:\n",
    "        print(f\"Repository: {sample['repo']}\")\n",
    "        print(f\"File Name: {sample['file_name']}\")\n",
    "        print(f\"Code Snippet:\\n{sample['code'][:100]}...\")  # Print the first 100 characters of the code\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"No Python files found in the scraped repositories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e39a3-1941-4dde-abf7-91e3fcf6ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_samples = []\n",
    "for sample in code_samples:\n",
    "     lines = sample[\"code\"].strip().split(\"\\n\")\n",
    "     if len(lines) > 10 and len(lines) <= 1000 and \"__init__\" not in sample[\"file_name\"]:\n",
    "         cleaned_samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e47af8-1ec7-4baf-a23c-957e1ae91698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    repo                   file_name  \\\n",
      "0  minimaxir/big-list-of-naughty-strings      scripts/txt_to_json.py   \n",
      "1                   google-research/bert  create_pretraining_data.py   \n",
      "2                   google-research/bert         extract_features.py   \n",
      "3                   google-research/bert                 modeling.py   \n",
      "4                   google-research/bert            modeling_test.py   \n",
      "\n",
      "                                                code  \n",
      "0  ###\\tQuick Python Script to convert the Big Li...  \n",
      "1  # coding=utf-8\\n# Copyright 2018 The Google AI...  \n",
      "2  # coding=utf-8\\n# Copyright 2018 The Google AI...  \n",
      "3  # coding=utf-8\\n# Copyright 2018 The Google AI...  \n",
      "4  # coding=utf-8\\n# Copyright 2018 The Google AI...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " # Convert the cleaned data to a DataFrame\n",
    "df = pd.DataFrame(cleaned_samples)\n",
    " # Save the code snippets to a CSV file\n",
    "df.to_csv(\"github_code_snippets.csv\", index=False)\n",
    " # Verify saved data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b380f768-64fd-45e6-847b-8c3c75f93054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " # Add a delay after scraping each repository\n",
    "time.sleep(1)  # Wait for 1 second between requests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
